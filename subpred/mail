Ich habe jetzt die Ergebnisse generiert über die wir gesprochen hatten, wo ich die GO terms nach verschiedenen Kriterien gefiltert habe um zu gucken ob das die Korrelationen zwischen den F1 scores und den Ähnlichkeits-Maßen verändert.

Zuerst hatte ich die Idee, die GO-term-Paare in zwei Klassen einzuteilen: Die mit einer F1 score unter 0.75, und die mit einer F1 score >= 0.75. Daduch bekomme ich mehr als 30 samples pro Klasse. Dann wollte ich mit statistischen Tests, z.B. einem single-sample-t-test, herrausfinden ob die Unterschiede in den Ähnlichkeitsmaßen statistisch signifikant sind zwischen den zwei Gruppen. Da sind 13 von 22 similarity scores statistisch signifikant, nach der Bonferroni-Korrektur. Ein t-test macht natürlich nur Sinn wenn die Variable auch normalverteilt ist, also habe ich noch einen Shapiro-Wilk-Test durchgeführt. Am besten abgeschnitten hat da die mittlere Sequenzidentität, die hat eine Shapiro-Wilk statistik von 0.987 (1 ist die perfekte Normalverteilung) und einen korrigierten p-Value von 0.031 für die Nullhypothese dass die Sequenzidentität bei Paaren mit niedrigen F1 scores nicht geringer ist. Die GO Semantic Similarity scores schneiden z.B. auch sehr gut ab in den Tests.

Vorhersagen lässt sich eine niedrige Score aus den similarity measures per SVM mit einer F1 score von 0.68. Bei dem SVM-Modell habe ich allerdings noch nicht die Hyperparameter optimiert oder feature selection ausprobiert, da könnte man evtl. noch 0.1-0.2 mehr rausholen. Außerdem könnte man so sagen, welche Features am wichtigsten waren. Für das SVM-Modell habe ich alle Maße aufeinmal benutzt

Dann habe ich noch simple lineare Regressions-Modelle versucht, ein Modell pro Ähnlichkeitsmaß. Da waren die R^2 values allerdings relativ nahe an 0, der höchste Absolutwert war 0.16.

Nun habe ich noch die GO terms nach vielen verschiedenen Kriterien gefiltert, um zu gucken ob die durchschnittlichen F1 scores und deren Spearman-Korrelationen mit den similarity scores höher werden. Das funktioniert auch ganz gut. Wenn ich z.B. nur GO terms betrachte die mit ChEBI terms verlinkt sind dann sinkt die Anzahl der Klassen von 36 auf 23, die mean test score steigt von 0.875 auf 0.885, un die Korrelation mit der minimalen Sequenzidentität steigt von 0.507 auf 0.535. Da will ich noch alle Kombinationen ausprobieren von GO-Filtern und Korrelations-Koeffizienzen, und die ließen sich auch miteinander kombinieren. Dadurch hätte ich eine Kombination aus Ähnlichkeits-Scores, mit dem sich eine gute Menge an Klassen finden ließe.


Ich habe nun noch die SVM optimiert. Wenn ich alle Ähnlichkeitsmaße als Features benutze dann bekomme ich eine F1 score von 0.713, wenn ich nur die similarities mit einem shapiro wilk test von >= 0.9 nehme dann sind es 0.77. In beiden Fällen wurden fast alle Features ausgewählt, Jiang-Similarity in beiden Fällen nicht.

Dann habe ich noch alle Mengen an GO terms generiert für verschiedene Filterkriterien. GO terms habe ich gefiltert nach Degree, In-Degree, Out-Degree, das Level in der Hierarchie, die Zahl der annotierten Proteine, den Evidence-Codes, und danach ob der GO term einen Link zu Chebi hat.

Für jeden GO-term-Filter habe ich mir dann angeguckt, wie viele GO terms noch übrig bleiben, wie viele Proteine mit denen abgedeckt werden, was der maximale Unterschied im Bezug auf die Korrelationskoeffizienten ist, und Unterschiede in der durchschnittlichen F1 score.

Ein Beispiel für einen guten Filter wäre *go_ids_level_le_3*, der alle GO terms entfernt die unter level 3 der Hierarchie sind. Damit verringert sich die Anzahl der Klassen von 36 auf 23, die Anzahl der möglichen Paare von 404 auf 158, und der Korrelationskoeffizient mit der minimalen Sequenzidentität steigt. Die F1 scores sinken um 0.02. Dafür wären mit diesen Klassen praktisch alle Proteine abgedeckt.

Idee 1: Was wenn ich mir nur level 3 alleine angucke und verwandte Terms entferne?

Idee 2: GO terms laut Ergebnissen von t-test filtern.

Notes:

Beob.: Fast immer gibt es positive Korrelation mit minimaler sequence alignment score, und negative mit maximaler.

Was ist das Endergebnis? Eine Pipeline, die eine Menge an Klassen für Klassifizierung generiert die gut funktioniert und

- Möglichst viele Proteine abdeckt
- Möglichst hohe Score hat bei Klassifikation


