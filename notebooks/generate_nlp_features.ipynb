{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conda environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create and activate conda env\n",
    "# biovec only works with python 3.6 because of old dependencies, here is conda env for that:\n",
    "\n",
    "# conda create -n biovec python=3.6 ipython pandas ipykernel\n",
    "# conda activate biovec\n",
    "# pip3 install biovec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz -O - | gunzip -c > swissprot.fasta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing biovec package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (takes a few hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biovec\n",
    "\n",
    "retrain = False\n",
    "if retrain:\n",
    "    pv = biovec.models.ProtVec(\"swissprot.fasta\", corpus_fname=\"output_corpusfile_path.txt\", workers=80)\n",
    "    pv.save('swissprot.protvec.model')\n",
    "pv = biovec.models.load_protvec('swissprot.protvec.model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_nlp_feature(sequence:str):\n",
    "    arrays = pv.to_vecs(sequence)\n",
    "    return pd.Series(np.concatenate(arrays))\n",
    "\n",
    "sequences = pd.read_table(\"/home/ad/biovec_test/uniprot_transporter_sequences.tsv\", index_col=0, squeeze=True)\n",
    "encoded = sequences.apply(get_nlp_feature)\n",
    "encoded.to_csv(\"/home/ad/biovec_test/uniprot_transporter_sequences_encoded.tsv\", sep=\"\\t\")\n",
    "encoded.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to write own code that uses Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# from biovec repo\n",
    "def split_ngrams(seq, n):\n",
    "    \"\"\"\n",
    "    'AGAMQSASM' => [['AGA', 'MQS', 'ASM'], ['GAM','QSA'], ['AMQ', 'SAS']]\n",
    "    \"\"\"\n",
    "    a, b, c = zip(*[iter(seq)]*n), zip(*[iter(seq[1:])]*n), zip(*[iter(seq[2:])]*n)\n",
    "    str_ngrams = []\n",
    "    for ngrams in [a,b,c]:\n",
    "        x = [\"\".join(ngram) for ngram in ngrams]\n",
    "        str_ngrams.append(x)\n",
    "    return str_ngrams\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Word2Vec:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are explained here in detail: https://www.youtube.com/watch?v=gQddtTdmG_8 . \n",
    "\n",
    "In short, the algorithm is trained on a corpus, i.e. a text. After training, it can produce a vector of length n (for example n=100) for each word. The similarity between two words is then calculated through the cosine distance, which is given as dot(X,Y)/(norm(X)*norm(Y)), where X and Y are the vectors for a given word/ngram/kmer and norm() is the euclidean norm. \n",
    "\n",
    "A higher similarity score means that the words are more similar, in the context of the text that was fed to the neural network.\n",
    "\n",
    "The idea behind the Biovec package is to transform a sequence of amino acids into three vectors of 3-mers. Each starts at a different sequence position (0,1,2). This is probably supposed to teach the kmer order to the algorithm.\n",
    "\n",
    "Every sequence is then treated as a \"sentence\", where the words are the kmers in the sequence. One of the standard methods for calculating the similarity between two sentences is to simply add their vectors/embeddings together, and then calculate the cosine distance between the resulting vectors. Here is an example, using normal words and sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0 vs s1 -> 0.965923011302948\n",
      "s0 vs s2 -> 0.8659112453460693\n",
      "s0 vs s3 -> 0.5877998471260071\n"
     ]
    }
   ],
   "source": [
    "# example: from https://stackoverflow.com/a/66127454\n",
    "\n",
    "from scipy import spatial\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\") #choose from multiple models https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "s0 = 'Mark zuckerberg owns the facebook company'\n",
    "s1 = 'Facebook company ceo is mark zuckerberg'\n",
    "s2 = 'Microsoft is owned by Bill gates'\n",
    "s3 = 'How to learn japanese'\n",
    "\n",
    "def preprocess(s):\n",
    "    return [i.lower() for i in s.split()]\n",
    "\n",
    "def get_vector(s):\n",
    "    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)\n",
    "\n",
    "def get_similarity(v0,v1):\n",
    "    return 1 - spatial.distance.cosine(v0, v1)\n",
    "\n",
    "print('s0 vs s1 ->',1 - spatial.distance.cosine(get_vector(s0), get_vector(s1)))\n",
    "print('s0 vs s2 ->', 1 - spatial.distance.cosine(get_vector(s0), get_vector(s2)))\n",
    "print('s0 vs s3 ->', 1 - spatial.distance.cosine(get_vector(s0), get_vector(s3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models and datasets are from a github repository that lists example datasets. The number at the end stands for the vector size. The example above uses vectors of length 50, the highest number seems to be 300. This seems to be a tradeoff between storage space/computation time, and accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative that has not been tried before would be to use Doc2Vec, which is available in the same python package (gensim). TODO I should try that. This post explains Doc2Vec, and also a Word2Vec solution that used Cosine similarity and average vectors: https://datascience.stackexchange.com/a/23998"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word movers distance algorithm is another method, and is also included in the gensim package: https://datascience.stackexchange.com/a/31497"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, another score I came across was the DICE score, which is a variant of the F1 score. I would create a set of kmers, then divide the number of kmers in the intersection set by the total number of kmers. This would not even need any word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subpred4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8de6dd8f2d395005c42468f5d13cdd089376f2ad37eab4b2481f4d8703f0b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
