{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires Biovec environment, see clustering notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mamba create -n biovec python=3.6 ipython pandas ipykernel\n",
    "# mamba activate biovec\n",
    "# pip3 install biovec\n",
    "# make package (might throw some errors since ancient version of python)\n",
    "\n",
    "# ipython\n",
    "# import biovec\n",
    "# pv = biovec.models.ProtVec(\"swissprot.fasta\", corpus_fname=\"output_corpusfile_path.txt\", workers=80)\n",
    "\n",
    "# import biovec\n",
    "\n",
    "# pv = biovec.models.ProtVec(\n",
    "#     \"some_fasta_file.fasta\", corpus_fname=\"output_corpusfile_path.txt\"\n",
    "# )\n",
    "\n",
    "# # The n-gram \"QAT\" should be trained in advance\n",
    "# pv[\"QAT\"]\n",
    "\n",
    "# # convert whole amino acid sequence into vector\n",
    "# pv.to_vecs(\"ATATQSQSMTEEL\")\n",
    "\n",
    "# # save trained model into file\n",
    "# pv.save(\"model_file_path\")\n",
    "\n",
    "# # load trained model from file\n",
    "# pv2 = biovec.models.load_protvec(\"model_file_path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sequences = pd.read_table(\"/home/ad/biovec_test/uniprot_transporter_sequences.tsv\", index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biovec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = biovec.models.load_protvec(\"/home/ad/biovec_test/swissprot.protvec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def split_ngrams(seq, n):\n",
    "    \"\"\"\n",
    "    'AGAMQSASM' => [['AGA', 'MQS', 'ASM'], ['GAM','QSA'], ['AMQ', 'SAS']]\n",
    "    \"\"\"\n",
    "    a, b, c = zip(*[iter(seq)]*n), zip(*[iter(seq[1:])]*n), zip(*[iter(seq[2:])]*n)\n",
    "    str_ngrams = []\n",
    "    for ngrams in [a,b,c]:\n",
    "        x = [\"\".join(ngram) for ngram in ngrams]\n",
    "        str_ngrams.append(x)\n",
    "    return str_ngrams\n",
    "\n",
    "# def generate_corpusfile(fasta_fname, n, corpus_fname):\n",
    "#     '''\n",
    "#     Args:\n",
    "#         fasta_fname: corpus file name\n",
    "#         n: the number of chunks to split. In other words, \"n\" for \"n-gram\"\n",
    "#         corpus_fname: corpus_fnameput corpus file path\n",
    "#     Description:\n",
    "#         Protvec uses word2vec inside, and it requires to load corpus file\n",
    "#         to generate corpus.\n",
    "#     '''\n",
    "#     f = open(corpus_fname, \"w\")\n",
    "#     fasta = Fasta(fasta_fname)\n",
    "#     for record_id in tqdm(fasta.keys(), desc='corpus generation progress'):\n",
    "#         r = fasta[record_id]\n",
    "#         seq = str(r)\n",
    "#         ngram_patterns = split_ngrams(seq, n)\n",
    "#         for ngram_pattern in ngram_patterns:\n",
    "#             f.write(\" \".join(ngram_pattern) + \"\\n\")\n",
    "#     f.close()\n",
    "\n",
    "# def to_vecs(seq, size=100, n=3):\n",
    "#     \"\"\"\n",
    "#     convert sequence to three n-length vectors\n",
    "#     e.g. 'AGAMQSASM' => [ array([  ... * 100 ], array([  ... * 100 ], array([  ... * 100 ] ]\n",
    "#     \"\"\"\n",
    "#     seq = seq.replace(\"*\",\"\") # for sequences with * to indicate stop\n",
    "#     ngram_patterns = split_ngrams(seq, n)\n",
    "#     protvecs = np.zeros([3,size]) # numpy for easy integration with ML libraries\n",
    "#     for num in range(3):\n",
    "#         ngram_vecs = []\n",
    "#         for ngram in ngram_patterns[num]:\n",
    "#             try:\n",
    "#                 ngram_vecs.append(pv[ngram])\n",
    "#             except:\n",
    "#                 raise Exception(\"Model has never trained this n-gram: \" + ngram)\n",
    "#         protvecs[num,] = sum(ngram_vecs)\n",
    "#     return protvecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model_w2v = word2vec.Word2Vec.load(\"/home/ad/biovec_test/swissprot.protvec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MALQSQASEEAKGPWQEADQEQQEPVGSPEPESEPEPEPEPEPVPVPPPEPQPEPQPLPDPAPLPELEFESERVHEPEPTPTVETRGTARGFQPPEGGFGWVVVFAATWCNGSIFGIHNSVGILYSMLLEEEKEKNRQVEFQAAWVGALAMGMIFFCSPIVSIFTDRLGCRITATAGAAVAFIGLHTSSFTSSLSLRYFTYGILFGCGCSFAFQPSLVILGHYFQRRLGLANGVVSAGSSIFSMSFPFLIRMLGDKIKLAQTFQVLSTFMFVLMLLSLTYRPLLPSSQDTPSKRGVRTLHQRFLAQLRKYFNMRVFRQRTYRIWAFGIAAAALGYFVPYVHLMKYVEEEFSEIKETWVLLVCIGATSGLGRLVSGHISDSIPGLKKIYLQVLSFLLLGLMSMMIPLCRDFGGLIVVCLFLGLCDGFFITIMAPIAFELVGPMQASQAIGYLLGMMALPMIAGPPIAGLLRNCFGDYHVAFYFAGVPPIIGAVILFFVPLMHQRMFKKEQRDSSKDKMLAPDPDPNGELLPGSPNPEEPI'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequence = sequences[0]\n",
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectors = split_ngrams(test_sequence, 3)  # Three vectors, for the three starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vector1 = ngram_vectors[0]  # kmers in sequence when starting at 0 and sliding by three "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MAL'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vector1_ngram1 = ngram_vector1[0]\n",
    "ngram_vector1_ngram1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QSQ'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vector1_ngram2 = ngram_vector1[1]\n",
    "ngram_vector1_ngram2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.18808679e-01, -3.64876986e-01,  6.47983193e-01, -1.46426350e-01,\n",
       "       -1.52609244e-01, -1.66515931e-01, -3.83192301e-01, -1.84395611e-01,\n",
       "       -2.08178714e-01, -2.30770335e-01, -6.94512352e-02, -4.52766428e-04,\n",
       "        4.31878418e-02,  1.23070590e-01, -8.74646232e-02, -3.87230255e-02,\n",
       "        2.65492380e-01,  5.03719375e-02,  3.54006499e-01,  6.21168278e-02,\n",
       "       -2.57201105e-01, -1.36489691e-02, -1.99244395e-02,  4.79546823e-02,\n",
       "        1.71690788e-02, -5.52854389e-02,  9.23806354e-02,  7.55709736e-03,\n",
       "        5.39546832e-03,  7.81023037e-03,  3.50544862e-02, -1.86856706e-02,\n",
       "       -4.65532169e-02, -1.07445650e-01,  1.13820970e-01, -7.78736696e-02,\n",
       "       -5.66957742e-02,  7.97994137e-02, -1.19515814e-01,  4.82424311e-02,\n",
       "        1.71776205e-01, -1.00432143e-01,  2.24059612e-01, -9.02400762e-02,\n",
       "       -9.13605988e-02,  1.33866414e-01, -1.31028533e-01, -3.13403383e-02,\n",
       "       -1.58041149e-01, -1.74196381e-02, -1.17266186e-01, -8.67590979e-02,\n",
       "       -1.10433914e-01, -3.23376395e-02,  4.25140373e-02,  2.48912554e-02,\n",
       "        3.25932242e-02, -4.61270958e-02, -1.30247558e-02, -7.46370852e-02,\n",
       "       -6.44109920e-02,  8.49091038e-02, -9.19130594e-02,  9.16798636e-02,\n",
       "        5.54146096e-02,  2.85399053e-02, -2.02891044e-03, -2.79527381e-02,\n",
       "        7.48129142e-03, -1.23317130e-02, -2.01508403e-01,  1.55753475e-02,\n",
       "       -9.40807611e-02,  4.34938259e-02, -4.36071232e-02, -1.21257789e-01,\n",
       "       -3.31076868e-02,  2.78437138e-02,  7.24417195e-02,  1.14125103e-01,\n",
       "        8.37745517e-02,  4.75685038e-02, -1.31496146e-01,  6.07706606e-02,\n",
       "        5.40355444e-02, -3.37737985e-02, -8.05663541e-02, -5.93005195e-02,\n",
       "        1.18876614e-01,  2.05339968e-01, -8.08799639e-02,  5.51356487e-02,\n",
       "       -8.48053843e-02,  1.05128184e-01, -1.18352100e-01,  8.00203308e-02,\n",
       "        2.24378556e-01, -9.98510122e-02,  1.04930915e-01,  1.15974236e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv[ngram_vector1_ngram1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13461933, -0.7111297 ,  1.0208125 , -0.30877602, -0.23616222,\n",
       "       -0.25844306, -0.50489485, -0.08331859, -0.56245625, -0.31021148,\n",
       "        0.0801694 ,  0.04676345, -0.45370948,  0.3514357 , -0.32759288,\n",
       "        0.08846295,  0.6903758 , -0.06264126,  0.2731142 ,  0.18539931,\n",
       "       -0.4407096 , -0.04989775, -0.09461653, -0.11871418,  0.11407162,\n",
       "       -0.0691107 ,  0.14646347,  0.19423391,  0.14325903, -0.02893887,\n",
       "        0.10630041, -0.10056195,  0.04517817, -0.27524903,  0.00641009,\n",
       "       -0.04310326, -0.20991428,  0.27244735, -0.00301873, -0.06236536,\n",
       "        0.17510992, -0.16946386,  0.46349233, -0.16256922,  0.1544082 ,\n",
       "        0.3841244 , -0.31079298, -0.08728868, -0.08097062, -0.14748202,\n",
       "       -0.20406985, -0.13692127, -0.3161991 , -0.23075272,  0.14425012,\n",
       "       -0.01757712,  0.0135788 ,  0.14987619,  0.2340453 , -0.21841547,\n",
       "       -0.07607324,  0.12445412, -0.27735892,  0.09093606,  0.13038597,\n",
       "       -0.06298643,  0.10943196,  0.02477781,  0.06615185, -0.00410948,\n",
       "       -0.21076685, -0.1273542 , -0.15693507,  0.20575352, -0.03524854,\n",
       "       -0.13381761, -0.05984258, -0.15183052,  0.01404925,  0.37676117,\n",
       "        0.2597065 ,  0.03788966, -0.13252106,  0.22457005, -0.02704567,\n",
       "       -0.27321675, -0.14370263,  0.17767882,  0.21408625,  0.35924873,\n",
       "       -0.08593026,  0.02172395, -0.18345402,  0.05491377, -0.23799555,\n",
       "        0.00918958, -0.07896957, -0.04972139,  0.4285298 , -0.289105  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([model_w2v.wv[ngram_vector1_ngram1], model_w2v.wv[ngram_vector1_ngram2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are explained here in detail: https://www.youtube.com/watch?v=gQddtTdmG_8 . \n",
    "\n",
    "In short, the algorithm is trained on a corpus, i.e. a text. After training, it can produce a vector of length n (for example n=100) for each word. The similarity between two words is then calculated through the cosine distance, which is given as dot(X,Y)/(norm(X)*norm(Y)), where X and Y are the vectors for a given word/ngram/kmer and norm() is the euclidean norm. \n",
    "\n",
    "A higher similarity score means that the words are more similar, in the context of the text that was fed to the neural network.\n",
    "\n",
    "The idea behind the Biovec package is to transform a sequence of amino acids into three vectors of 3-mers. Each starts at a different sequence position (0,1,2). This is probably supposed to teach the kmer order to the algorithm.\n",
    "\n",
    "Every sequence is then treated as a \"sentence\", where the words are the kmers in the sequence. One of the standard methods for calculating the similarity between two sentences is to simply add their vectors/embeddings together, and then calculate the cosine distance between the resulting vectors. Here is an example, using normal words and sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: from https://stackoverflow.com/a/66127454\n",
    "\n",
    "from scipy import spatial\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\") #choose from multiple models https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0 vs s1 -> 0.965923011302948\n",
      "s0 vs s2 -> 0.8659112453460693\n",
      "s0 vs s3 -> 0.5877998471260071\n"
     ]
    }
   ],
   "source": [
    "s0 = 'Mark zuckerberg owns the facebook company'\n",
    "s1 = 'Facebook company ceo is mark zuckerberg'\n",
    "s2 = 'Microsoft is owned by Bill gates'\n",
    "s3 = 'How to learn japanese'\n",
    "\n",
    "def preprocess(s):\n",
    "    return [i.lower() for i in s.split()]\n",
    "\n",
    "def get_vector(s):\n",
    "    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)\n",
    "\n",
    "def get_similarity(v0,v1):\n",
    "    return 1 - spatial.distance.cosine(v0, v1)\n",
    "\n",
    "print('s0 vs s1 ->',1 - spatial.distance.cosine(get_vector(s0), get_vector(s1)))\n",
    "print('s0 vs s2 ->', 1 - spatial.distance.cosine(get_vector(s0), get_vector(s2)))\n",
    "print('s0 vs s3 ->', 1 - spatial.distance.cosine(get_vector(s0), get_vector(s3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models and datasets are from a github repository that lists example datasets. The number at the end stands for the vector size. The example above uses vectors of length 50, the highest number seems to be 300. This seems to be a tradeoff between storage space/computation time, and accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative that has not been tried before would be to use Doc2Vec, which is available in the same python package (gensim). TODO I should try that. This post explains Doc2Vec, and also a Word2Vec solution that used Cosine similarity and average vectors: https://datascience.stackexchange.com/a/23998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(get_vector(s1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word movers distance algorithm is another method, and is also included in the gensim package: https://datascience.stackexchange.com/a/31497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, another score I came across was the DICE score, which is a variant of the F1 score. I would create a set of kmers, then divide the number of kmers in the intersection set by the total number of kmers. This would not even need any word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding sequences with biovec package:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training (takes a few hours!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biovec\n",
    "\n",
    "retrain = False\n",
    "if retrain:\n",
    "    pv = biovec.models.ProtVec(\"/home/ad/biovec_test/swissprot.fasta\", corpus_fname=\"/home/ad/biovec_test/output_corpusfile_path.txt\", workers=80)\n",
    "    pv.save('/home/ad/biovec_test/swissprot.protvec.model')\n",
    "pv = biovec.models.load_protvec('swissprot.protvec.model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_feature(sequence:str):\n",
    "    arrays = pv.to_vecs(sequence)\n",
    "    return pd.Series(np.concatenate(arrays))\n",
    "encoded = sequences.apply(get_nlp_feature)\n",
    "encoded.to_csv(\"/home/ad/biovec_test/uniprot_transporter_sequences_encoded.tsv\", sep=\"\\t\")\n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uniprot</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P36021</th>\n",
       "      <td>-7.273644</td>\n",
       "      <td>-34.285099</td>\n",
       "      <td>68.556938</td>\n",
       "      <td>-12.244725</td>\n",
       "      <td>-5.441298</td>\n",
       "      <td>-10.215006</td>\n",
       "      <td>-68.510017</td>\n",
       "      <td>-22.670380</td>\n",
       "      <td>-39.208435</td>\n",
       "      <td>-11.681757</td>\n",
       "      <td>...</td>\n",
       "      <td>4.140173</td>\n",
       "      <td>7.689297</td>\n",
       "      <td>-8.704616</td>\n",
       "      <td>-3.346005</td>\n",
       "      <td>2.328844</td>\n",
       "      <td>-5.019221</td>\n",
       "      <td>1.109853</td>\n",
       "      <td>16.568235</td>\n",
       "      <td>20.324194</td>\n",
       "      <td>-6.998810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P43005</th>\n",
       "      <td>-4.140399</td>\n",
       "      <td>-32.438766</td>\n",
       "      <td>63.623219</td>\n",
       "      <td>-14.523251</td>\n",
       "      <td>-6.829222</td>\n",
       "      <td>-10.790448</td>\n",
       "      <td>-71.488716</td>\n",
       "      <td>-19.858337</td>\n",
       "      <td>-35.338825</td>\n",
       "      <td>-8.385507</td>\n",
       "      <td>...</td>\n",
       "      <td>3.039154</td>\n",
       "      <td>9.374497</td>\n",
       "      <td>-13.243600</td>\n",
       "      <td>-3.349254</td>\n",
       "      <td>-1.535459</td>\n",
       "      <td>-2.477201</td>\n",
       "      <td>2.961823</td>\n",
       "      <td>19.141970</td>\n",
       "      <td>21.797276</td>\n",
       "      <td>-5.176438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P43007</th>\n",
       "      <td>-6.950311</td>\n",
       "      <td>-34.846478</td>\n",
       "      <td>67.427254</td>\n",
       "      <td>-14.485823</td>\n",
       "      <td>-6.727311</td>\n",
       "      <td>-10.528156</td>\n",
       "      <td>-72.611015</td>\n",
       "      <td>-21.206213</td>\n",
       "      <td>-38.286049</td>\n",
       "      <td>-10.396502</td>\n",
       "      <td>...</td>\n",
       "      <td>7.083755</td>\n",
       "      <td>6.126781</td>\n",
       "      <td>-10.004407</td>\n",
       "      <td>-1.570559</td>\n",
       "      <td>0.785607</td>\n",
       "      <td>-4.293931</td>\n",
       "      <td>1.667595</td>\n",
       "      <td>14.724697</td>\n",
       "      <td>17.068716</td>\n",
       "      <td>-5.410892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5TGU0</th>\n",
       "      <td>-3.028285</td>\n",
       "      <td>-12.994875</td>\n",
       "      <td>22.860323</td>\n",
       "      <td>-1.317772</td>\n",
       "      <td>-4.241808</td>\n",
       "      <td>-4.403067</td>\n",
       "      <td>-20.944134</td>\n",
       "      <td>-6.393130</td>\n",
       "      <td>-10.412485</td>\n",
       "      <td>-1.996316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.936083</td>\n",
       "      <td>0.124781</td>\n",
       "      <td>-2.916309</td>\n",
       "      <td>1.244631</td>\n",
       "      <td>-0.117676</td>\n",
       "      <td>-2.666773</td>\n",
       "      <td>2.721554</td>\n",
       "      <td>3.414848</td>\n",
       "      <td>5.361672</td>\n",
       "      <td>-2.029417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q92536</th>\n",
       "      <td>-5.437946</td>\n",
       "      <td>-33.504353</td>\n",
       "      <td>63.424423</td>\n",
       "      <td>-9.342864</td>\n",
       "      <td>-9.211921</td>\n",
       "      <td>-9.999936</td>\n",
       "      <td>-67.682892</td>\n",
       "      <td>-19.497953</td>\n",
       "      <td>-34.540096</td>\n",
       "      <td>-8.593810</td>\n",
       "      <td>...</td>\n",
       "      <td>6.935112</td>\n",
       "      <td>7.029308</td>\n",
       "      <td>-11.998138</td>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.094206</td>\n",
       "      <td>-2.755319</td>\n",
       "      <td>4.371674</td>\n",
       "      <td>15.640297</td>\n",
       "      <td>18.742008</td>\n",
       "      <td>-6.791428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3         4          5    \\\n",
       "Uniprot                                                                   \n",
       "P36021  -7.273644 -34.285099  68.556938 -12.244725 -5.441298 -10.215006   \n",
       "P43005  -4.140399 -32.438766  63.623219 -14.523251 -6.829222 -10.790448   \n",
       "P43007  -6.950311 -34.846478  67.427254 -14.485823 -6.727311 -10.528156   \n",
       "Q5TGU0  -3.028285 -12.994875  22.860323  -1.317772 -4.241808  -4.403067   \n",
       "Q92536  -5.437946 -33.504353  63.424423  -9.342864 -9.211921  -9.999936   \n",
       "\n",
       "               6          7          8          9    ...       290       291  \\\n",
       "Uniprot                                              ...                       \n",
       "P36021  -68.510017 -22.670380 -39.208435 -11.681757  ...  4.140173  7.689297   \n",
       "P43005  -71.488716 -19.858337 -35.338825  -8.385507  ...  3.039154  9.374497   \n",
       "P43007  -72.611015 -21.206213 -38.286049 -10.396502  ...  7.083755  6.126781   \n",
       "Q5TGU0  -20.944134  -6.393130 -10.412485  -1.996316  ...  1.936083  0.124781   \n",
       "Q92536  -67.682892 -19.497953 -34.540096  -8.593810  ...  6.935112  7.029308   \n",
       "\n",
       "               292       293       294       295       296        297  \\\n",
       "Uniprot                                                                 \n",
       "P36021   -8.704616 -3.346005  2.328844 -5.019221  1.109853  16.568235   \n",
       "P43005  -13.243600 -3.349254 -1.535459 -2.477201  2.961823  19.141970   \n",
       "P43007  -10.004407 -1.570559  0.785607 -4.293931  1.667595  14.724697   \n",
       "Q5TGU0   -2.916309  1.244631 -0.117676 -2.666773  2.721554   3.414848   \n",
       "Q92536  -11.998138  0.645455  0.094206 -2.755319  4.371674  15.640297   \n",
       "\n",
       "               298       299  \n",
       "Uniprot                       \n",
       "P36021   20.324194 -6.998810  \n",
       "P43005   21.797276 -5.176438  \n",
       "P43007   17.068716 -5.410892  \n",
       "Q5TGU0    5.361672 -2.029417  \n",
       "Q92536   18.742008 -6.791428  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biovec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fc381ae223073bfee0800c4ce20123532e7acd55fee1b4063fec7d2cc833561"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
